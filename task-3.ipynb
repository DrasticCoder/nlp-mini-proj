{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install simpletransformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# data loading - user upload file, stratified split\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "from google.colab import files\n",
    "uploaded = files.upload()\n",
    "\n",
    "# load file from user upload\n",
    "data = pd.read_csv(next(iter(uploaded)), encoding='ISO-8859-1')\n",
    "\n",
    "# stratified split\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "for train_idx, val_idx in split.split(data, data[\"Prediction\"]):\n",
    "    train_data = data.iloc[train_idx]\n",
    "    val_data   = data.iloc[val_idx]\n",
    "\n",
    "# create df for train and val\n",
    "train_df = pd.DataFrame({\n",
    "    'text': train_data['Input'],\n",
    "    'labels': train_data['Prediction']\n",
    "})\n",
    "val_df = pd.DataFrame({\n",
    "    'text': val_data['Input'],\n",
    "    'labels': val_data['Prediction']\n",
    "})\n",
    "\n",
    "print(\"train labels:\", train_df['labels'].nunique())\n",
    "print(\"val labels:\", val_df['labels'].nunique())\n",
    "print(train_df.head())\n",
    "print(val_df.head())\n",
    "\n",
    "print(\"unique cats:\", data[\"Prediction\"].unique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Text Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# text processing - clean text\n",
    "import re\n",
    "\n",
    "def clean_text(txt):\n",
    "    txt = txt.lower()\n",
    "    txt = re.sub(r'[^a-zA-Z\\s]', '', txt)\n",
    "    txt = txt.strip()\n",
    "    return txt\n",
    "\n",
    "train_df['text'] = train_df['text'].apply(clean_text)\n",
    "val_df['text']   = val_df['text'].apply(clean_text)\n",
    "\n",
    "print(train_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# model init - create bert & roberta models using simpletransformers\n",
    "from simpletransformers.classification import ClassificationModel\n",
    "\n",
    "bert_model = ClassificationModel('bert', 'bert-base-uncased', num_labels=len(data[\"Prediction\"].unique()), use_cuda=True)\n",
    "roberta_model = ClassificationModel('roberta', 'roberta-base', num_labels=len(data[\"Prediction\"].unique()), use_cuda=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Model Training with BERT and RoBERTa\n",
    "Basic Model Training\n",
    "Train the BERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# model train prep - encode labels, set args, train models\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from simpletransformers.classification import ClassificationArgs\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "all_lbls = list(train_df[\"labels\"]) + list(val_df[\"labels\"])\n",
    "label_encoder.fit(all_lbls)\n",
    "\n",
    "train_df[\"labels\"] = label_encoder.transform(train_df[\"labels\"])\n",
    "val_df[\"labels\"]   = label_encoder.transform(val_df[\"labels\"])\n",
    "\n",
    "model_args = ClassificationArgs(\n",
    "    num_train_epochs=3,\n",
    "    train_batch_size=8,\n",
    "    eval_batch_size=8,\n",
    "    learning_rate=3e-5,\n",
    "    max_seq_length=128,\n",
    "    weight_decay=0.01,\n",
    "    warmup_steps=0,\n",
    "    logging_steps=50,\n",
    "    save_steps=200,\n",
    "    overwrite_output_dir=True,\n",
    "    output_dir='outputs'\n",
    ")\n",
    "\n",
    "bert_model = ClassificationModel('bert', 'bert-base-uncased', num_labels=len(label_encoder.classes_), args=model_args, use_cuda=True)\n",
    "bert_model.train_model(train_df)\n",
    "\n",
    "roberta_model = ClassificationModel('roberta', 'roberta-base', num_labels=len(label_encoder.classes_), args=model_args, use_cuda=True)\n",
    "roberta_model.train_model(train_df)\n",
    "\n",
    "print(\"bert & roberta models trained succesfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Evaluation on Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# evaluation on validation set\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "\n",
    "# eval bert\n",
    "result_bert, out_bert, wrong_bert = bert_model.eval_model(val_df)\n",
    "bert_preds = np.argmax(out_bert, axis=1)\n",
    "bert_preds_lbl = label_encoder.inverse_transform(bert_preds)\n",
    "val_df['bert_pred'] = bert_preds_lbl\n",
    "\n",
    "print(\"bert eval result:\")\n",
    "print(result_bert)\n",
    "target_names = [str(x) for x in label_encoder.classes_]\n",
    "print(\"\\nbert clas report:\")\n",
    "print(classification_report(val_df['labels'], bert_preds, target_names=target_names))\n",
    "\n",
    "# eval roberta\n",
    "result_roberta, out_roberta, wrong_roberta = roberta_model.eval_model(val_df)\n",
    "roberta_preds = np.argmax(out_roberta, axis=1)\n",
    "roberta_preds_lbl = label_encoder.inverse_transform(roberta_preds)\n",
    "val_df['roberta_pred'] = roberta_preds_lbl\n",
    "\n",
    "print(\"\\nroberta eval result:\")\n",
    "print(result_roberta)\n",
    "print(\"\\nroberta clas report:\")\n",
    "print(classification_report(val_df['labels'], roberta_preds, target_names=target_names))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# make table of eval metrics\n",
    "import pandas as pd\n",
    "\n",
    "data_dict = {\n",
    "    \"no.\": [1, 2],\n",
    "    \"model name\": [\"bert\", \"roberta\"],\n",
    "    \"precision\": [0.61, 0.62],\n",
    "    \"recall\": [0.48, 0.52],\n",
    "    \"f1 score\": [0.51, 0.55],\n",
    "    \"accuracy\": [0.84, 0.85],\n",
    "    \"mcc\": [0.8158, 0.8223]\n",
    "}\n",
    "\n",
    "eval_df = pd.DataFrame(data_dict)\n",
    "print(eval_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "6. Saving the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# save model - save bert and roberta manually\n",
    "bert_model.model.save_pretrained(\"bert_model\")\n",
    "bert_model.tokenizer.save_pretrained(\"bert_model\")\n",
    "print(\"bert model saved manually!\")\n",
    "\n",
    "roberta_model.model.save_pretrained(\"roberta_model\")\n",
    "roberta_model.tokenizer.save_pretrained(\"roberta_model\")\n",
    "print(\"roberta model saved manually!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "7. Prediction on Real-World Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# prediction on real-world input\n",
    "import torch\n",
    "from simpletransformers.classification import ClassificationModel\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def clean_text(txt):\n",
    "    txt = txt.lower()\n",
    "    txt = re.sub(r'[^a-zA-Z\\s]', '', txt)\n",
    "    txt = txt.strip()\n",
    "    return txt\n",
    "\n",
    "# load saved models; use use_cuda false if no gpu\n",
    "bert_model = ClassificationModel(\"bert\", \"bert_model\", use_cuda=False)\n",
    "roberta_model = ClassificationModel(\"roberta\", \"roberta_model\", use_cuda=False)\n",
    "\n",
    "# set label encoder classes manually\n",
    "encoder = LabelEncoder()\n",
    "encoder.classes_ = np.array(['automotive', 'beauty & personal care', 'books', 'clothing', 'electronics',\n",
    "                              'furniture', 'gaming', 'grocery', 'health & wellness', 'home appliances',\n",
    "                              'jewelry & accessories', 'office supplies', 'pet supplies', 'sports & outdoors', 'toys'])\n",
    "\n",
    "input_text = input(\"enter product description: \")\n",
    "cleaned_text = clean_text(input_text)\n",
    "\n",
    "bert_pred, _ = bert_model.predict([cleaned_text])\n",
    "roberta_pred, _ = roberta_model.predict([cleaned_text])\n",
    "\n",
    "bert_lbl = encoder.inverse_transform(np.array(bert_pred))[0]\n",
    "roberta_lbl = encoder.inverse_transform(np.array(roberta_pred))[0]\n",
    "\n",
    "print(\"\\nbert predicted category:\", bert_lbl)\n",
    "print(\"roberta predicted category:\", roberta_lbl)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
