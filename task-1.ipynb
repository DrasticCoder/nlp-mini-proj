{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Section 1: Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from google.colab import files\n",
    "\n",
    "# csv format \"Input\" , \"Prediction\n",
    "uploaded = files.upload()\n",
    "\n",
    "df = pd.read_csv(next(iter(uploaded)))\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Section 2: Exploratory Data Analysis (EDA)\n",
    "\n",
    "check basic info, summary statistics, class distribution, and missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print(df.info())\n",
    "print(df.describe())\n",
    "\n",
    "# visualizing class distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(x='Prediction', data=df)\n",
    "plt.title(\"Distribution of Online Course Categories\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "# missing value check\n",
    "print(\"Missing values in each column:\\n\", df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Section 3: Text Processing\n",
    "\n",
    "clean the course description text from the Input column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def clean_text(text):\n",
    "    # cut non alphabetic characters numbers newlines etc and convert them to lowercase\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text, flags=re.I|re.A)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = re.sub(r'\\n', ' ', text)\n",
    "    return text.lower().strip()\n",
    "\n",
    "# clean text form \"Input\" column\n",
    "df['cleaned_text'] = df['Input'].apply(clean_text)\n",
    "df[['Input', 'cleaned_text']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Section 4: Generate NLP-Based Features\n",
    "We derive simple numeric features from the cleaned text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def generate_nlp_features(df):\n",
    "    # char count\n",
    "    df['char_count'] = df['cleaned_text'].apply(len)\n",
    "    df['word_count'] = df['cleaned_text'].apply(lambda x: len(x.split()))\n",
    "    df['avg_word_length'] = df['cleaned_text'].apply(lambda x: np.mean([len(word) for word in x.split()]) if x.split() else 0)\n",
    "    df['stopword_count'] = df['cleaned_text'].apply(lambda x: len([word for word in x.split() if word in stop_words]))\n",
    "    df['htag_count'] = df['cleaned_text'].apply(lambda x: x.count('#'))\n",
    "    return df\n",
    "\n",
    "df = generate_nlp_features(df)\n",
    "df[['char_count', 'word_count', 'avg_word_length', 'stopword_count', 'htag_count']].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Section 5: Generate Text Embedding Features\n",
    "\n",
    "features using Bag of Words, TFIDF, and FastText embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# installs fasttext if not done already\n",
    "!pip install fasttext\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import fasttext\n",
    "import os\n",
    "\n",
    "# downloads pretrained fasttext model\n",
    "if not os.path.exists('cc.en.300.bin'):\n",
    "    !wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.bin.gz\n",
    "    !gunzip cc.en.300.bin.gz\n",
    "\n",
    "# bog of words\n",
    "bow_vectorizer = CountVectorizer(max_features=5000)\n",
    "X_bow = bow_vectorizer.fit_transform(df['cleaned_text'])\n",
    "\n",
    "# tfidf\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(df['cleaned_text'])\n",
    "# fasttext embeddings load pretrained fs model\n",
    "ft_model = fasttext.load_model('cc.en.300.bin')\n",
    "\n",
    "def get_fasttext_embeddings(text):\n",
    "    words = text.split()\n",
    "    if words:\n",
    "        word_vectors = [ft_model.get_word_vector(word) for word in words]\n",
    "        return np.mean(word_vectors, axis=0)\n",
    "    else:\n",
    "        return np.zeros(ft_model.get_dimension())\n",
    "\n",
    "df['fasttext_embeddings'] = df['cleaned_text'].apply(get_fasttext_embeddings)\n",
    "# testing one rows embedding\n",
    "print(\"FastText embedding shape:\", df['fasttext_embeddings'].iloc[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Section 6: Train Supervised Models on NLP-Based Features We use the numeric features generated earlier for training classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# select nlp features  &target\n",
    "X_nlp = df[['char_count', 'word_count', 'avg_word_length', 'stopword_count', 'htag_count']]\n",
    "y = df['Prediction']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_nlp, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#using Random Forest\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "print(\"Random Forest Classifier Report (NLP Features):\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Section 7: Train Models on BoW, TFIDF, and FastText Features\n",
    "\n",
    "use define a helper function to train several classifiers and compare there performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tabulate\n",
    "\n",
    "import joblib\n",
    "from tabulate import tabulate\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def train_and_evaluate(X_train, X_test, y_train, y_test, feature_name):\n",
    "    models = {\n",
    "        'Logistic Regression': LogisticRegression(multi_class='ovr', max_iter=1000),\n",
    "        'SVM': SVC(kernel='linear'),\n",
    "        'Decision Tree': DecisionTreeClassifier(),\n",
    "        'KNN': KNeighborsClassifier(),\n",
    "        'Naive Bayes': MultinomialNB(),\n",
    "        'Random Forest': RandomForestClassifier(random_state=42)\n",
    "    }\n",
    "\n",
    "    results = {}\n",
    "    for name, model in models.items():\n",
    "        print(f\"\\nTraining {name} on {feature_name} features...\")\n",
    "        if name == 'Naive Bayes' and feature_name == \"fasttext\":\n",
    "            scaler = MinMaxScaler()\n",
    "            X_train = scaler.fit_transform(X_train) # fits and transform training data\n",
    "            X_test = scaler.transform(X_test) #transform test data using fittedd scaler\n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "        joblib.dump(model, f'{name}_{feature_name}_features.pkl')\n",
    "        y_pred = model.predict(X_test)\n",
    "        results[name] = classification_report(y_test, y_pred, output_dict=True)\n",
    "        print(f\"{name} Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "    # create &display  summary table\n",
    "    summary = pd.DataFrame({model: results[model]['weighted avg'] for model in results}).T\n",
    "    summary = summary[['precision', 'recall', 'f1-score']]\n",
    "    print(\"\\nModel Comparison for\", feature_name, \"features:\")\n",
    "    print(tabulate(summary, headers='keys', tablefmt='grid'))\n",
    "    return results\n",
    "\n",
    "# train on baw features\n",
    "X_train_bow, X_test_bow, y_train_bow, y_test_bow = train_test_split(X_bow, y, test_size=0.2, random_state=42)\n",
    "results_bow = train_and_evaluate(X_train_bow, X_test_bow, y_train_bow, y_test_bow, \"bow\")\n",
    "\n",
    "#trains on tfidf feature\n",
    "X_train_tfidf, X_test_tfidf, y_train_tfidf, y_test_tfidf = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)\n",
    "results_tfidf = train_and_evaluate(X_train_tfidf, X_test_tfidf, y_train_tfidf, y_test_tfidf, \"tfidf\")\n",
    "\n",
    "# fast text stack embeddings form 2D array\n",
    "fasttext_embeddings = np.vstack(df['fasttext_embeddings'])\n",
    "X_train_fasttext, X_test_fasttext, y_train_fasttext, y_test_fasttext = train_test_split(fasttext_embeddings, y, test_size=0.2, random_state=42)\n",
    "results_fasttext = train_and_evaluate(X_train_fasttext, X_test_fasttext, y_train_fasttext, y_test_fasttext, \"fasttext\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Section 8: Combined Features (NLP + TFIDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import hstack, csr_matrix\n",
    "\n",
    "# combines nlp feature (with tfidf\n",
    "nlp_sparse = csr_matrix(X_nlp.values)\n",
    "X_combined = hstack([nlp_sparse, X_tfidf])\n",
    "\n",
    "X_train_comb, X_test_comb, y_train_comb, y_test_comb = train_test_split(X_combined, y, test_size=0.2, random_state=42)\n",
    "results_combined = train_and_evaluate(X_train_comb, X_test_comb, y_train_comb, y_test_comb, \"Combined_TFIDF\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Section 9: Save Models and Make Real-World Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import joblib\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "def combination_features(text, tfidf_vectorizer):\n",
    "    \"\"\"Combine NLP features and TFIDF features for a given text.\"\"\"\n",
    "    text_cleaned = clean_text(text)\n",
    "    temp_df = pd.DataFrame({'cleaned_text': [text_cleaned]})\n",
    "    temp_df = generate_nlp_features(temp_df)\n",
    "\n",
    "    from scipy.sparse import csr_matrix, hstack\n",
    "    nlp_feats = temp_df[['char_count', 'word_count', 'avg_word_length', 'stopword_count', 'htag_count']].values\n",
    "    nlp_sparse = csr_matrix(nlp_feats)\n",
    "\n",
    "    tfidf_feats = tfidf_vectorizer.transform([text_cleaned])\n",
    "    combined = hstack([nlp_sparse, tfidf_feats])\n",
    "    print(\"Combined feature shape:\", combined.shape)\n",
    "    return combined\n",
    "\n",
    "new_text = input(\"Enter an online course description to classify: \")\n",
    "new_text_cleaned = clean_text(new_text)\n",
    "\n",
    "new_text_features_bow = bow_vectorizer.transform([new_text_cleaned])\n",
    "new_text_features_tfidf = tfidf_vectorizer.transform([new_text_cleaned])\n",
    "new_text_features_combined = combination_features(new_text, tfidf_vectorizer)\n",
    "\n",
    "model_dir = '/content/'\n",
    "model_files = [f for f in os.listdir(model_dir) if f.endswith('.pkl')]\n",
    "\n",
    "#regex pattern\n",
    "patterns = {\n",
    "    'bow': re.compile(r'bow', re.IGNORECASE),\n",
    "    'tfidf': re.compile(r'tfidf', re.IGNORECASE),\n",
    "    'fasttext': re.compile(r'fasttext', re.IGNORECASE),\n",
    "    'combined': re.compile(r'combined', re.IGNORECASE)\n",
    "}\n",
    "\n",
    "print(\"\\n--- Model Predictions ---\")\n",
    "if not model_files:\n",
    "    print(\"No model files found.\")\n",
    "else:\n",
    "    for model_file in model_files:\n",
    "        print(f\"\\nLoading model: {model_file}\")\n",
    "        loaded_model = joblib.load(os.path.join(model_dir, model_file))\n",
    "        try:\n",
    "            if patterns['bow'].search(model_file):\n",
    "                prediction = loaded_model.predict(new_text_features_bow)\n",
    "                print(f\"Prediction using BoW model ({model_file}): {prediction}\")\n",
    "            elif patterns['tfidf'].search(model_file):\n",
    "                prediction = loaded_model.predict(new_text_features_tfidf)\n",
    "                print(f\"Prediction using TFIDF model ({model_file}): {prediction}\")\n",
    "            elif patterns['combined'].search(model_file):\n",
    "                prediction = loaded_model.predict(new_text_features_combined)\n",
    "                print(f\"Prediction using Combined (NLP + TFIDF) model ({model_file}): {prediction}\")\n",
    "            elif patterns['fasttext'].search(model_file):\n",
    "                new_text_fasttext = np.array([get_fasttext_embeddings(new_text_cleaned)])\n",
    "                prediction = loaded_model.predict(new_text_fasttext)\n",
    "                print(f\"Prediction using FastText model ({model_file}): {prediction}\")\n",
    "            else:\n",
    "                print(f\"Unknown or unsupported model type in file: {model_file}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error predicting with model {model_file}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Section 10: Analysis of Results After training and testing, compare the model performances. (For example, you might note that certain features or combinations perform better for categorizing online courses. Update your analysis as needed based on your dataset’s outcomes.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tabulate import tabulate\n",
    "\n",
    "#function to extract &format summary metrix\n",
    "def extract_summary(results_dict):\n",
    "    summary = {}\n",
    "    for model, metrics in results_dict.items():\n",
    "        accuracy = metrics.get('accuracy', 0)\n",
    "        note = \"\"\n",
    "        #flags if accuracy is exactly 1 bcoz of dataset issue overfit\n",
    "        if accuracy == 1.0:\n",
    "            note = \"Potential data issue/overfitting\"\n",
    "        summary[model] = {\n",
    "            \"precision\": round(metrics['weighted avg']['precision'], 3),\n",
    "            \"recall\": round(metrics['weighted avg']['recall'], 3),\n",
    "            \"f1-score\": round(metrics['weighted avg']['f1-score'], 3),\n",
    "            \"accuracy\": round(accuracy, 3),\n",
    "            \"note\": note\n",
    "        }\n",
    "    return pd.DataFrame(summary).T\n",
    "\n",
    "#results_bow,results_tfidf,results_fasttext,results_combined\n",
    "df_bow = extract_summary(results_bow) if 'results_bow' in globals() else pd.DataFrame()\n",
    "df_tfidf = extract_summary(results_tfidf) if 'results_tfidf' in globals() else pd.DataFrame()\n",
    "df_fasttext = extract_summary(results_fasttext) if 'results_fasttext' in globals() else pd.DataFrame()\n",
    "df_combined = extract_summary(results_combined) if 'results_combined' in globals() else pd.DataFrame()\n",
    "\n",
    "print(\"Analysis of Model Performance\\n\")\n",
    "\n",
    "if not df_bow.empty:\n",
    "    print(\" Bag of Words Features\")\n",
    "    print(tabulate(df_bow, headers='keys', tablefmt='grid'))\n",
    "    print(\"\\n\")\n",
    "\n",
    "if not df_tfidf.empty:\n",
    "    print(\" TFIDF Features\")\n",
    "    print(tabulate(df_tfidf, headers='keys', tablefmt='grid'))\n",
    "    print(\"\\n\")\n",
    "\n",
    "if not df_fasttext.empty:\n",
    "    print(\" FastText Features\")\n",
    "    print(tabulate(df_fasttext, headers='keys', tablefmt='grid'))\n",
    "    print(\"\\n\")\n",
    "\n",
    "if not df_combined.empty:\n",
    "    print(\"Combined (NLP + TFIDF) Features\")\n",
    "    print(tabulate(df_combined, headers='keys', tablefmt='grid'))\n",
    "    print(\"\\n\")\n",
    "\n",
    "print(\"Overall Analysis:\")\n",
    "print(\"\"\"\n",
    "- Data Quality Check: If any model shows an accuracy of 1.0, this may indicate that the dataset is too easy, overfitting has occurred, or the labels are not properly distributed. Re-examine the dataset for potential errors.\n",
    "- Model Performance:\n",
    "   - Models trained on Bag of Words and TFIDF features generally demonstrate robust performance.\n",
    "   - FastText-based models, while promising, show variability across classifiers, suggesting further hyperparameter tuning or more training data may be needed.\n",
    "   - The combined feature approach (NLP + TFIDF) enriches the feature set but does not guarantee uniformly high performance across all models.\n",
    "\"\"\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
