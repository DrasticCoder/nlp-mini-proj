{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 0: install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall -y numpy pandas\n",
    "!pip install numpy==1.26.0 pandas==2.2.2\n",
    "!pip install nltk keras gensim scikit-learn contractions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2 of Mini Project: Text Classification using Deep Learning\n",
    "1. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload ur dataset, must contain columns \"input\" for course descriptions and \"prediction\" for course categories\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from google.colab import files\n",
    "uploaded  = files.upload()\n",
    "\n",
    "# replace filename with actual uploaded file name\n",
    "data = pd.read_csv(next(iter(uploaded)))\n",
    "\n",
    "print(\"dataset overview:\")\n",
    "print(data.info())\n",
    "print(\"\\n sample data:\")\n",
    "print(data.head())\n",
    "\n",
    "# plot class distribution\n",
    "plt.figure(figsize=(8,5))\n",
    "data['Prediction'].value_counts().plot(kind='bar', color='skyblue')\n",
    "plt.title('course category distribution')\n",
    "plt.xlabel('course category')\n",
    "plt.ylabel('count')\n",
    "plt.show()\n",
    "\n",
    "# split data into train, validation, test sets\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "train_data, val_data  = train_test_split(train_data, test_size=0.1, random_state=42)\n",
    "\n",
    "# plot pie chart for overall class distribution\n",
    "class_distribution = data['Prediction'].value_counts()\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.pie(class_distribution, labels=class_distribution.index, autopct='%1.1f%%', startangle=140)\n",
    "plt.title(\"course category distribution\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import contractions\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_description(text):\n",
    "    # expand contractions\n",
    "    text = contractions.fix(text)\n",
    "    # remove special characters numbers, convert lowercase\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    text = text.lower().strip()\n",
    "    # tokenize text\n",
    "    words = word_tokenize(text)\n",
    "    # remove stopwords and lemmatize; ignore words shorter than 3 characters\n",
    "    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words and len(word) > 2]\n",
    "    return ' '.join(words)\n",
    "\n",
    "train_data['Processed_Description'] = train_data['Input'].apply(preprocess_description)\n",
    "val_data['Processed_Description']   = val_data['Input'].apply(preprocess_description)\n",
    "test_data['Processed_Description']  = test_data['Input'].apply(preprocess_description)\n",
    "\n",
    "print(\"\\npreprocessed sample:\")\n",
    "print(train_data[['Input', 'Processed_Description']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Text Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# bow vectorizer setup\n",
    "bow_vectorizer = CountVectorizer(max_features=5000)\n",
    "X_train_bow = bow_vectorizer.fit_transform(train_data['Processed_Description']).toarray()\n",
    "X_val_bow   = bow_vectorizer.transform(val_data['Processed_Description']).toarray()\n",
    "X_test_bow  = bow_vectorizer.transform(test_data['Processed_Description']).toarray()\n",
    "\n",
    "# tfidf vectorizer setup\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(train_data['Processed_Description']).toarray()\n",
    "X_val_tfidf   = tfidf_vectorizer.transform(val_data['Processed_Description']).toarray()\n",
    "X_test_tfidf  = tfidf_vectorizer.transform(test_data['Processed_Description']).toarray()\n",
    "\n",
    "# tokenize texts for fasttext/word2vec; convert processed description into list words\n",
    "train_tokens = train_data['Processed_Description'].apply(str.split)\n",
    "val_tokens   = val_data['Processed_Description'].apply(str.split)\n",
    "test_tokens  = test_data['Processed_Description'].apply(str.split)\n",
    "\n",
    "# fasttext embeddings; train on our corpus\n",
    "fasttext_model = gensim.models.FastText(train_tokens, vector_size=100, window=5, min_count=5)\n",
    "X_train_fasttext = np.array([ np.mean([ fasttext_model.wv[word] for word in tokens if word in fasttext_model.wv ] or [np.zeros(100)], axis=0)\n",
    "                              for tokens in train_tokens])\n",
    "X_val_fasttext   = np.array([ np.mean([ fasttext_model.wv[word] for word in tokens if word in fasttext_model.wv ] or [np.zeros(100)], axis=0)\n",
    "                              for tokens in val_tokens])\n",
    "X_test_fasttext  = np.array([ np.mean([ fasttext_model.wv[word] for word in tokens if word in fasttext_model.wv ] or [np.zeros(100)], axis=0)\n",
    "                              for tokens in test_tokens])\n",
    "\n",
    "# word2vec embeddings; custom trained\n",
    "word2vec_model = gensim.models.Word2Vec(sentences=train_tokens, vector_size=100, window=5, min_count=5)\n",
    "X_train_word2vec = np.array([ np.mean([ word2vec_model.wv[word] for word in tokens if word in word2vec_model.wv ] or [np.zeros(100)], axis=0)\n",
    "                              for tokens in train_tokens])\n",
    "X_val_word2vec   = np.array([ np.mean([ word2vec_model.wv[word] for word in tokens if word in word2vec_model.wv ] or [np.zeros(100)], axis=0)\n",
    "                              for tokens in val_tokens])\n",
    "X_test_word2vec  = np.array([ np.mean([ word2vec_model.wv[word] for word in tokens if word in word2vec_model.wv ] or [np.zeros(100)], axis=0)\n",
    "                              for tokens in test_tokens])\n",
    "\n",
    "# bert embeddings\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "4. Model Training with Different Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model training with deep learnin architectures\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Conv1D, MaxPooling1D, LSTM, Bidirectional, Embedding, Flatten, MultiHeadAttention, LayerNormalization, Add, Input\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "# encode labels\n",
    "encoder = LabelEncoder()\n",
    "y_train = encoder.fit_transform(train_data['Prediction'])\n",
    "y_val   = encoder.transform(val_data['Prediction'])\n",
    "y_test  = encoder.transform(test_data['Prediction'])\n",
    "\n",
    "# define diff models\n",
    "def cnn_model(input_dim, num_classes):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim, 128, input_length=input_dim))\n",
    "    model.add(Conv1D(filters=64, kernel_size=5, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    opt = Adam(learning_rate=0.001)\n",
    "    model.compile(optimizer=opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def lstm_model(input_dim, num_classes):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim, 128, input_length=input_dim))\n",
    "    model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def cnn_bilstm_model(input_dim, num_classes):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim, 128, input_length=input_dim))\n",
    "    model.add(Conv1D(filters=64, kernel_size=5, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Bidirectional(LSTM(100, dropout=0.2, recurrent_dropout=0.2)))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def transformer_model(input_dim, num_classes):\n",
    "    inp = Input(shape=(input_dim,))\n",
    "    emb = Embedding(input_dim=input_dim, output_dim=128)(inp)\n",
    "    x   = LSTM(100, return_sequences=True)(emb)\n",
    "    att = MultiHeadAttention(num_heads=2, key_dim=64)(x, x)\n",
    "    att = LayerNormalization()(att)\n",
    "    x   = Add()([x, att])\n",
    "    x   = Flatten()(x)\n",
    "    x   = Dense(100, activation='relu')(x)\n",
    "    out = Dense(num_classes, activation='softmax')(x)\n",
    "    model = Model(inputs=inp, outputs=out)\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def fully_connected_model(input_dim, num_classes):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim, 128, input_length=input_dim))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# set input dims based on embed type\n",
    "# for models using embedding layer, use fixed input dim (like 5000)\n",
    "input_dim_fixed = 5000\n",
    "num_classes = len(encoder.classes_)\n",
    "\n",
    "# for vectorizer/transformer generated embeddings use their dims\n",
    "input_dim_bow       = X_train_bow.shape[1]\n",
    "input_dim_tfidf     = X_train_tfidf.shape[1]\n",
    "input_dim_fasttext  = X_train_fasttext.shape[1]\n",
    "input_dim_word2vec  = X_train_word2vec.shape[1]\n",
    "input_dim_bert      = X_train_bert.shape[1]\n",
    "\n",
    "# lr scheduller\n",
    "lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, verbose=1)\n",
    "\n",
    "# train cnn model using bow features (eg)\n",
    "cnn = cnn_model(input_dim_bow, num_classes)\n",
    "cnn.fit(X_train_bow, y_train, validation_data=(X_val_bow, y_val), epochs=5, batch_size=64, callbacks=[lr_scheduler])\n",
    "\n",
    "# train lstm model using bow features\n",
    "lstm = lstm_model(input_dim_fixed, num_classes)\n",
    "lstm.fit(X_train_bow, y_train, validation_data=(X_val_bow, y_val), epochs=5, batch_size=64)\n",
    "\n",
    "# train cnn-bilstm model using bow features\n",
    "cnn_bilstm = cnn_bilstm_model(input_dim_fixed, num_classes)\n",
    "cnn_bilstm.fit(X_train_bow, y_train, validation_data=(X_val_bow, y_val), epochs=5, batch_size=64)\n",
    "\n",
    "# train transformer model using word2vec embeddings\n",
    "transformer_word2vec = transformer_model(input_dim_word2vec, num_classes)\n",
    "transformer_word2vec.fit(X_train_word2vec, y_train, validation_data=(X_val_word2vec, y_val), epochs=5, batch_size=64)\n",
    "\n",
    "# train fully connected model using bert embeddings\n",
    "fully_connected_bert = fully_connected_model(input_dim_bert, num_classes)\n",
    "fully_connected_bert.fit(X_train_bert, y_train, validation_data=(X_val_bert, y_val), epochs=5, batch_size=64)\n",
    "\n",
    "print(\"all models trained succesfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Comparative Analysis of Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# evaluate cnn model\n",
    "y_pred_cnn = cnn.predict(X_test_bow)\n",
    "cnn_acc = accuracy_score(y_test, y_pred_cnn.argmax(axis=1))\n",
    "print(\"cnn acc:\", cnn_acc)\n",
    "print(\"cnn clas report:\\n\", classification_report(y_test, y_pred_cnn.argmax(axis=1), zero_division=0))\n",
    "\n",
    "# evaluate lstm model\n",
    "y_pred_lstm = lstm.predict(X_test_bow)\n",
    "lstm_acc = accuracy_score(y_test, y_pred_lstm.argmax(axis=1))\n",
    "print(\"lstm acc:\", lstm_acc)\n",
    "print(\"lstm clas report:\\n\", classification_report(y_test, y_pred_lstm.argmax(axis=1), zero_division=0))\n",
    "\n",
    "# evaluate cnn-bilstm model\n",
    "y_pred_cnn_bilstm = cnn_bilstm.predict(X_test_bow)\n",
    "cnn_bilstm_acc = accuracy_score(y_test, y_pred_cnn_bilstm.argmax(axis=1))\n",
    "print(\"cnn-bilstm acc:\", cnn_bilstm_acc)\n",
    "print(\"cnn-bilstm clas report:\\n\", classification_report(y_test, y_pred_cnn_bilstm.argmax(axis=1), zero_division=0))\n",
    "\n",
    "# evaluate transformer (word2vec) model\n",
    "y_pred_transformer = transformer_word2vec.predict(X_test_word2vec)\n",
    "transformer_acc = accuracy_score(y_test, y_pred_transformer.argmax(axis=1))\n",
    "print(\"transformer (word2vec) acc:\", transformer_acc)\n",
    "print(\"transformer (word2vec) clas report:\\n\", classification_report(y_test, y_pred_transformer.argmax(axis=1), zero_division=0))\n",
    "\n",
    "# evaluate fully connected (bert) model\n",
    "y_pred_fc_bert = fully_connected_bert.predict(X_test_bert)\n",
    "fc_bert_acc = accuracy_score(y_test, y_pred_fc_bert.argmax(axis=1))\n",
    "print(\"fully connected (bert) acc:\", fc_bert_acc)\n",
    "print(\"fully connected (bert) clas report:\\n\", classification_report(y_test, y_pred_fc_bert.argmax(axis=1), zero_division=0))\n",
    "\n",
    "# summarise results in table\n",
    "results = {\n",
    "    \"model\": [\"cnn\", \"lstm\", \"cnn-bilstm\", \"transformer (word2vec)\", \"fully connected (bert)\"],\n",
    "    \"acc\": [cnn_acc, lstm_acc, cnn_bilstm_acc, transformer_acc, fc_bert_acc]\n",
    "}\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\n model performance summary:\")\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Saving the Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# save best model (cnn perfmed best\n",
    "cnn.save('best_online_course_classifier.h5')\n",
    "print(\"best model saved as 'best_online_course_classifier.h5'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "7. Prediction on Real-World Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "import numpy as np\n",
    "\n",
    "# load saved model\n",
    "model = load_model('best_online_course_classifier.h5')\n",
    "\n",
    "# new input: update course description if needed\n",
    "new_text = \"Learn Python programming with real-world projects and interactive coding exercises.\"\n",
    "# preproc new input\n",
    "processed_text = preprocess_description(new_text)\n",
    "# convert to bow features (if best model trained on bow)\n",
    "new_text_features = bow_vectorizer.transform([processed_text]).toarray()\n",
    "\n",
    "# predict class\n",
    "predictions = model.predict(new_text_features)\n",
    "predicted_class = encoder.inverse_transform([predictions.argmax(axis=1)[0]])[0]\n",
    "confidence = predictions.max() * 100\n",
    "print(f\"predicted course category: {predicted_class} (confidence: {confidence:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
